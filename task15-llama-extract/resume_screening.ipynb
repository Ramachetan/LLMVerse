{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from Resumes\n",
    "\n",
    "Let us assume that we are running a hiring process for a company and we have received a list of resumes from candidates. We want to extract structured data from the resumes so that we can run a screening process and shortlist candidates. \n",
    "\n",
    "Take a look at one of the resumes in the `data/resumes` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"./data/resumes/ai_researcher.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x107c23e00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"./data/resumes/ai_researcher.pdf\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that all the resumes have different layouts but contain common information like name, email, experience, education, etc. \n",
    "\n",
    "With LlamaExtract, we will show you how to:\n",
    "- *Define* a data schema to extract the information of interest. \n",
    "- *Iterate* over the data schema to generalize the schema for multiple resumes.\n",
    "- *Finalize* the schema and schedule extractions for multiple resumes.\n",
    "\n",
    "We will start by defining a `LlamaExtract` client which provides a Python interface to the LlamaExtract API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_cloud_services\n",
      "  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama_cloud_services) (8.1.8)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.17 (from llama_cloud_services)\n",
      "  Downloading llama_cloud-0.1.17-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting llama-index-core>=0.11.0 (from llama_cloud_services)\n",
      "  Downloading llama_index_core-0.12.27-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama_cloud_services) (4.3.7)\n",
      "Requirement already satisfied: pydantic!=2.10 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama_cloud_services) (2.11.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama_cloud_services) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (2025.1.31)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (0.28.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (6.0.2)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Downloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached aiohttp-3.11.14-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Downloading banks-2.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting dataclasses-json (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (2025.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (2.2.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (11.1.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from llama-index-core>=0.11.0->llama_cloud_services) (4.13.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from pydantic!=2.10->llama_cloud_services) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from pydantic!=2.10->llama_cloud_services) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from pydantic!=2.10->llama_cloud_services) (0.4.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached multidict-6.2.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Downloading griffe-1.7.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core>=0.11.0->llama_cloud_services) (3.1.6)\n",
      "Requirement already satisfied: anyio in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (0.14.0)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from nltk>3.8.1->llama-index-core>=0.11.0->llama_cloud_services) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama_cloud_services) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from requests>=2.31.0->llama-index-core>=0.11.0->llama_cloud_services) (2.3.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core>=0.11.0->llama_cloud_services) (24.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.20.0->llama-cloud<0.2.0,>=0.1.17->llama_cloud_services) (1.3.1)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core>=0.11.0->llama_cloud_services)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ajkumar/Documents/LLMVerse/.venv/lib/python3.13/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core>=0.11.0->llama_cloud_services) (3.0.2)\n",
      "Downloading llama_cloud_services-0.6.9-py3-none-any.whl (29 kB)\n",
      "Downloading llama_cloud-0.1.17-py3-none-any.whl (253 kB)\n",
      "Downloading llama_index_core-0.12.27-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.11.14-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Downloading banks-2.1.0-py3-none-any.whl (28 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading sqlalchemy-2.0.40-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.2.0-cp313-cp313-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl (91 kB)\n",
      "Downloading griffe-1.7.1-py3-none-any.whl (129 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: filetype, dirtyjson, wrapt, tenacity, SQLAlchemy, propcache, networkx, mypy-extensions, multidict, marshmallow, joblib, greenlet, frozenlist, colorama, attrs, aiohappyeyeballs, yarl, typing-inspect, nltk, griffe, deprecated, aiosignal, llama-cloud, dataclasses-json, banks, aiohttp, llama-index-core, llama_cloud_services\n",
      "Successfully installed SQLAlchemy-2.0.40 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 banks-2.1.0 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.5.0 greenlet-3.1.1 griffe-1.7.1 joblib-1.4.2 llama-cloud-0.1.17 llama-index-core-0.12.27 llama_cloud_services-0.6.9 marshmallow-3.26.1 multidict-6.2.0 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 propcache-0.3.1 tenacity-9.0.0 typing-inspect-0.9.0 wrapt-1.17.2 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip install llama_cloud_services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No project_id provided, fetching default project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Creating extraction jobs: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Extracting files: 100%|██████████| 1/1 [00:13<00:00, 13.54s/it]\n",
      "Uploading files: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Creating extraction jobs: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Extracting files: 100%|██████████| 1/1 [00:25<00:00, 25.83s/it]\n",
      "Uploading files: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Creating extraction jobs: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Extracting files: 100%|██████████| 1/1 [00:04<00:00,  4.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_cloud_services import LlamaExtract\n",
    "\n",
    "\n",
    "# Load environment variables (put LLAMA_CLOUD_API_KEY in your .env file)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Optionally, add your project id/organization id\n",
    "llama_extract = LlamaExtract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the data schema\n",
    "\n",
    "Next, let us try to extract two fields from the resume: `name` and `email`. We can either use a Python dictionary structure to define the `data_schema` as a JSON or use a Pydantic model instead, for brevity and convenience. In either case, our output is guaranteed to validate against this schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"The name of the candidate\")\n",
    "    email: str = Field(description=\"The email address of the candidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud.core.api_error import ApiError\n",
    "\n",
    "try:\n",
    "    existing_agent = llama_extract.get_agent(name=\"resume-screening\")\n",
    "    if existing_agent:\n",
    "        llama_extract.delete_agent(existing_agent.id)\n",
    "except ApiError as e:\n",
    "    if e.status_code == 404:\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "agent = llama_extract.create_agent(name=\"resume-screening\", data_schema=Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExtractionAgent(id=f565c694-e79c-4cc9-a50f-92f1490c67cd, name=resume-screening)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_extract.list_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dr. Rachel Zhang', 'email': 'rachel.zhang@email.com'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = agent.extract(\"./data/resumes/ai_researcher.pdf\")\n",
    "resume.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over the data schema\n",
    "\n",
    "Now that we have created a data schema, let us add more fields to the schema. We will add `experience` and `education` fields to the schema. \n",
    "- We can create a new Pydantic model for each of these fields and represent `experience` and `education` as lists of these models. Doing this will allow us to extract multiple entities from the resume without having to pre-define how many experiences or education the candidate has. \n",
    "- We have added a `description` parameter to provide more context for extraction. We can use `description` to provide example inputs/outputs for the extraction. \n",
    "- Note that we have annotated the `start_date` and `end_date` fields with `Optional[str]` to indicate that these fields are optional. This is *important* because the schema will be used to extract data from multiple resumes and not all resumes will have the same format. A field must only be required if it is guaranteed to be present in all the resumes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class Education(BaseModel):\n",
    "    institution: str = Field(description=\"The institution of the candidate\")\n",
    "    degree: str = Field(description=\"The degree of the candidate\")\n",
    "    start_date: Optional[str] = Field(\n",
    "        default=None, description=\"The start date of the candidate's education\"\n",
    "    )\n",
    "    end_date: Optional[str] = Field(\n",
    "        default=None, description=\"The end date of the candidate's education\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    company: str = Field(description=\"The name of the company\")\n",
    "    title: str = Field(description=\"The title of the candidate\")\n",
    "    description: Optional[str] = Field(\n",
    "        default=None, description=\"The description of the candidate's experience\"\n",
    "    )\n",
    "    start_date: Optional[str] = Field(\n",
    "        default=None, description=\"The start date of the candidate's experience\"\n",
    "    )\n",
    "    end_date: Optional[str] = Field(\n",
    "        default=None, description=\"The end date of the candidate's experience\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"The name of the candidate\")\n",
    "    email: str = Field(description=\"The email address of the candidate\")\n",
    "    links: List[str] = Field(\n",
    "        description=\"The links to the candidate's social media profiles\"\n",
    "    )\n",
    "    experience: List[Experience] = Field(description=\"The candidate's experience\")\n",
    "    education: List[Education] = Field(description=\"The candidate's education\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will update the `data_schema` for the `resume-screening` agent to use the new `Resume` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dr. Rachel Zhang, Ph.D.',\n",
       " 'email': 'rachel.zhang@email.com',\n",
       " 'links': [],\n",
       " 'experience': [{'company': 'DeepMind',\n",
       "   'title': 'Senior Research Scientist',\n",
       "   'description': 'Lead researcher on large-scale multi-task learning systems, developing novel architectures that improve cross-task generalization by 40%. Pioneered new approach to zero-shot learning using contrastive training, published in NeurIPS 2023. Built and led team of 6 researchers working on foundational ML models. Developed novel regularization techniques for large language models, reducing catastrophic forgetting by 35%.',\n",
       "   'start_date': '2019',\n",
       "   'end_date': 'Present'},\n",
       "  {'company': 'Google Research',\n",
       "   'title': 'Research Scientist',\n",
       "   'description': 'Developed probabilistic frameworks for robust ML, published in ICML 2018. Created novel attention mechanisms for computer vision models, improving accuracy by 25%. Led collaboration with Google Brain team on efficient training methods for transformer models. Mentored 4 PhD interns and collaborated with academic institutions.',\n",
       "   'start_date': '2015',\n",
       "   'end_date': '2019'},\n",
       "  {'company': 'Columbia University',\n",
       "   'title': 'Research Assistant Professor',\n",
       "   'description': 'Published seminal work on Bayesian optimization methods (cited 1000+ times) Taught graduate-level courses in Machine Learning and Statistical Learning Theory Supervised 5 PhD students and 3 MSc students Secured $500K in research grants for probabilistic ML research',\n",
       "   'start_date': '2011',\n",
       "   'end_date': '2015'}],\n",
       " 'education': [{'institution': 'Columbia University',\n",
       "   'degree': 'Ph.D. in Computer Science',\n",
       "   'start_date': '2007',\n",
       "   'end_date': '2011'},\n",
       "  {'institution': 'Stanford University',\n",
       "   'degree': 'M.S. in Computer Science',\n",
       "   'start_date': '2005',\n",
       "   'end_date': '2007'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.data_schema = Resume\n",
    "resume = agent.extract(\"./data/resumes/ai_researcher.pdf\")\n",
    "resume.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start. Let us add a few more fields to the schema and re-run the extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalSkills(BaseModel):\n",
    "    programming_languages: List[str] = Field(\n",
    "        description=\"The programming languages the candidate is proficient in.\"\n",
    "    )\n",
    "    frameworks: List[str] = Field(\n",
    "        description=\"The tools/frameworks the candidate is proficient in, e.g. React, Django, PyTorch, etc.\"\n",
    "    )\n",
    "    skills: List[str] = Field(\n",
    "        description=\"Other general skills the candidate is proficient in, e.g. Data Engineering, Machine Learning, etc.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    name: str = Field(description=\"The name of the candidate\")\n",
    "    email: str = Field(description=\"The email address of the candidate\")\n",
    "    links: List[str] = Field(\n",
    "        description=\"The links to the candidate's social media profiles\"\n",
    "    )\n",
    "    experience: List[Experience] = Field(description=\"The candidate's experience\")\n",
    "    education: List[Education] = Field(description=\"The candidate's education\")\n",
    "    technical_skills: TechnicalSkills = Field(\n",
    "        description=\"The candidate's technical skills\"\n",
    "    )\n",
    "    key_accomplishments: str = Field(\n",
    "        description=\"Summarize the candidates highest achievements.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dr. Rachel Zhang, Ph.D.',\n",
       " 'email': 'rachel.zhang@email.com',\n",
       " 'links': [],\n",
       " 'experience': [{'company': 'DeepMind',\n",
       "   'title': 'Senior Research Scientist',\n",
       "   'description': 'Lead researcher on large-scale multi-task learning systems, developing novel architectures that improve cross-task generalization by 40%. Pioneered new approach to zero-shot learning using contrastive training, published in NeurIPS 2023. Built and led team of 6 researchers working on foundational ML models. Developed novel regularization techniques for large language models, reducing catastrophic forgetting by 35%.',\n",
       "   'start_date': '2019',\n",
       "   'end_date': 'Present'},\n",
       "  {'company': 'Google Research',\n",
       "   'title': 'Research Scientist',\n",
       "   'description': 'Developed probabilistic frameworks for robust ML, published in ICML 2018. Created novel attention mechanisms for computer vision models, improving accuracy by 25%. Led collaboration with Google Brain team on efficient training methods for transformer models. Mentored 4 PhD interns and collaborated with academic institutions.',\n",
       "   'start_date': '2015',\n",
       "   'end_date': '2019'},\n",
       "  {'company': 'Columbia University',\n",
       "   'title': 'Research Assistant Professor',\n",
       "   'description': 'Published seminal work on Bayesian optimization methods (cited 1000+ times). Taught graduate-level courses in Machine Learning and Statistical Learning Theory. Supervised 5 PhD students and 3 MSc students. Secured $500K in research grants for probabilistic ML research.',\n",
       "   'start_date': '2011',\n",
       "   'end_date': '2015'}],\n",
       " 'education': [{'institution': 'Columbia University',\n",
       "   'degree': 'Ph.D. in Computer Science',\n",
       "   'start_date': '2007',\n",
       "   'end_date': '2011'},\n",
       "  {'institution': 'Stanford University',\n",
       "   'degree': 'M.S. in Computer Science',\n",
       "   'start_date': '2005',\n",
       "   'end_date': '2007'}],\n",
       " 'technical_skills': {'programming_languages': ['Python',\n",
       "   'C++',\n",
       "   'Julia',\n",
       "   'CUDA'],\n",
       "  'frameworks': ['PyTorch', 'TensorFlow', 'JAX', 'Ray'],\n",
       "  'skills': ['Deep Learning',\n",
       "   'Reinforcement Learning',\n",
       "   'Probabilistic Models',\n",
       "   'Multi-Task Learning',\n",
       "   'Zero-Shot Learning',\n",
       "   'Neural Architecture Search']},\n",
       " 'key_accomplishments': 'AI researcher with 12+ years of experience spanning classical machine learning, deep learning, and probabilistic modeling. Led groundbreaking research in reinforcement learning, generative models, and multi-task learning. Published 25+ papers in top-tier conferences (NeurIPS, ICML, ICLR). Strong track record of transitioning theoretical advances into practical applications in both academic and industrial settings.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.data_schema = Resume\n",
    "resume = agent.extract(\"./data/resumes/ai_researcher.pdf\")\n",
    "resume.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing the schema\n",
    "\n",
    "This is great! We have extracted a lot of key information from the resume that is well-typed and can be used downstream for further processing. Until now, this data is ephemeral and will be lost if we close the session. Let us save the state of our extraction and use it to extract data from multiple resumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additionalProperties': False,\n",
       " 'properties': {'name': {'description': 'The name of the candidate',\n",
       "   'type': 'string'},\n",
       "  'email': {'description': 'The email address of the candidate',\n",
       "   'type': 'string'},\n",
       "  'links': {'description': \"The links to the candidate's social media profiles\",\n",
       "   'items': {'type': 'string'},\n",
       "   'type': 'array'},\n",
       "  'experience': {'description': \"The candidate's experience\",\n",
       "   'items': {'additionalProperties': False,\n",
       "    'properties': {'company': {'description': 'The name of the company',\n",
       "      'type': 'string'},\n",
       "     'title': {'description': 'The title of the candidate', 'type': 'string'},\n",
       "     'description': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'description': \"The description of the candidate's experience\"},\n",
       "     'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'description': \"The start date of the candidate's experience\"},\n",
       "     'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'description': \"The end date of the candidate's experience\"}},\n",
       "    'required': ['company', 'title', 'description', 'start_date', 'end_date'],\n",
       "    'type': 'object'},\n",
       "   'type': 'array'},\n",
       "  'education': {'description': \"The candidate's education\",\n",
       "   'items': {'additionalProperties': False,\n",
       "    'properties': {'institution': {'description': 'The institution of the candidate',\n",
       "      'type': 'string'},\n",
       "     'degree': {'description': 'The degree of the candidate',\n",
       "      'type': 'string'},\n",
       "     'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'description': \"The start date of the candidate's education\"},\n",
       "     'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "      'description': \"The end date of the candidate's education\"}},\n",
       "    'required': ['institution', 'degree', 'start_date', 'end_date'],\n",
       "    'type': 'object'},\n",
       "   'type': 'array'},\n",
       "  'technical_skills': {'additionalProperties': False,\n",
       "   'description': \"The candidate's technical skills\",\n",
       "   'properties': {'programming_languages': {'description': 'The programming languages the candidate is proficient in.',\n",
       "     'items': {'type': 'string'},\n",
       "     'type': 'array'},\n",
       "    'frameworks': {'description': 'The tools/frameworks the candidate is proficient in, e.g. React, Django, PyTorch, etc.',\n",
       "     'items': {'type': 'string'},\n",
       "     'type': 'array'},\n",
       "    'skills': {'description': 'Other general skills the candidate is proficient in, e.g. Data Engineering, Machine Learning, etc.',\n",
       "     'items': {'type': 'string'},\n",
       "     'type': 'array'}},\n",
       "   'required': ['programming_languages', 'frameworks', 'skills'],\n",
       "   'type': 'object'},\n",
       "  'key_accomplishments': {'description': 'Summarize the candidates highest achievements.',\n",
       "   'type': 'string'}},\n",
       " 'required': ['name',\n",
       "  'email',\n",
       "  'links',\n",
       "  'experience',\n",
       "  'education',\n",
       "  'technical_skills',\n",
       "  'key_accomplishments'],\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = llama_extract.get_agent(\"resume-screening\")\n",
    "agent.data_schema  # Latest schema should be returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queueing extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple resumes, we can use the `queue_extraction` method to run extractions asynchronously. This is ideal for processing batch extraction jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
      "Creating extraction jobs: 100%|██████████| 2/2 [00:00<00:00,  5.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# All resumes in the data/resumes directory\n",
    "resumes = []\n",
    "\n",
    "with os.scandir(\"./data/resumes\") as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file():\n",
    "            resumes.append(entry.path)\n",
    "\n",
    "jobs = await agent.queue_extraction(resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the latest status of the extractions for any `job_id`, we can use the `get_extraction_job` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<StatusEnum.PENDING: 'PENDING'>, <StatusEnum.PENDING: 'PENDING'>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[agent.get_extraction_job(job_id=job.id).status for job in jobs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that all extraction runs are in a PENDING state. We can check back again to see if the extractions have completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<StatusEnum.SUCCESS: 'SUCCESS'>, <StatusEnum.PENDING: 'PENDING'>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[agent.get_extraction_job(job_id=job.id).status for job in jobs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving results\n",
    "\n",
    "Let us now retrieve the results of the extractions. If the status of the extraction is `SUCCESS`, we can retrieve the data from the `data` field. In case there are errors (status = `ERROR`), we can retrieve the error message from the `error` field. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction status for job 8816d49c-537f-431f-a84b-1954201fbe57: ExtractState.PENDING\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for job in jobs:\n",
    "    extract_run = agent.get_extraction_run_for_job(job.id)\n",
    "    if extract_run.status == \"SUCCESS\":\n",
    "        results.append(extract_run.data)\n",
    "    else:\n",
    "        print(f\"Extraction status for job {job.id}: {extract_run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Dr. Rachel Zhang, Ph.D.',\n",
       " 'email': 'rachel.zhang@email.com',\n",
       " 'links': [],\n",
       " 'experience': [{'company': 'DeepMind',\n",
       "   'title': 'Senior Research Scientist',\n",
       "   'description': 'Lead researcher on large-scale multi-task learning systems, developing novel architectures that improve cross-task generalization by 40%. Pioneered new approach to zero-shot learning using contrastive training, published in NeurIPS 2023. Built and led team of 6 researchers working on foundational ML models. Developed novel regularization techniques for large language models, reducing catastrophic forgetting by 35%.',\n",
       "   'start_date': '2019',\n",
       "   'end_date': 'Present'},\n",
       "  {'company': 'Google Research',\n",
       "   'title': 'Research Scientist',\n",
       "   'description': 'Developed probabilistic frameworks for robust ML, published in ICML 2018. Created novel attention mechanisms for computer vision models, improving accuracy by 25%. Led collaboration with Google Brain team on efficient training methods for transformer models. Mentored 4 PhD interns and collaborated with academic institutions.',\n",
       "   'start_date': '2015',\n",
       "   'end_date': '2019'},\n",
       "  {'company': 'Columbia University',\n",
       "   'title': 'Research Assistant Professor',\n",
       "   'description': 'Published seminal work on Bayesian optimization methods (cited 1000+ times). Taught graduate-level courses in Machine Learning and Statistical Learning Theory. Supervised 5 PhD students and 3 MSc students. Secured $500K in research grants for probabilistic ML research.',\n",
       "   'start_date': '2011',\n",
       "   'end_date': '2015'}],\n",
       " 'education': [{'institution': 'Columbia University',\n",
       "   'degree': 'Ph.D. in Computer Science',\n",
       "   'start_date': '2007',\n",
       "   'end_date': '2011'},\n",
       "  {'institution': 'Stanford University',\n",
       "   'degree': 'M.S. in Computer Science',\n",
       "   'start_date': '2005',\n",
       "   'end_date': '2007'}],\n",
       " 'technical_skills': {'programming_languages': ['Python',\n",
       "   'C++',\n",
       "   'Julia',\n",
       "   'CUDA'],\n",
       "  'frameworks': ['PyTorch', 'TensorFlow', 'JAX', 'Ray'],\n",
       "  'skills': ['Deep Learning',\n",
       "   'Reinforcement Learning',\n",
       "   'Probabilistic Models',\n",
       "   'Multi-Task Learning',\n",
       "   'Zero-Shot Learning',\n",
       "   'Neural Architecture Search']},\n",
       " 'key_accomplishments': 'AI researcher with 12+ years of experience spanning classical machine learning, deep learning, and probabilistic modeling. Led groundbreaking research in reinforcement learning, generative models, and multi-task learning. Published 25+ papers in top-tier conferences (NeurIPS, ICML, ICLR). Strong track record of transitioning theoretical advances into practical applications in both academic and industrial settings.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Sarah Chen',\n",
       " 'email': 'sarah.chen@email.com',\n",
       " 'links': [],\n",
       " 'education': [{'degree': 'Master of Science in Computer Science',\n",
       "   'end_date': '2013',\n",
       "   'start_date': None,\n",
       "   'institution': 'Stanford University'},\n",
       "  {'degree': 'Bachelor of Science in Computer Engineering',\n",
       "   'end_date': '2011',\n",
       "   'start_date': None,\n",
       "   'institution': 'University of California, Berkeley'}],\n",
       " 'experience': [{'title': 'Senior Software Architect',\n",
       "   'company': 'TechCorp Solutions',\n",
       "   'end_date': None,\n",
       "   'start_date': '2020',\n",
       "   'description': '- Led architectural design and implementation of a cloud-native platform serving 2M+ users\\n- Established architectural guidelines and best practices adopted across 12 development teams\\n- Reduced system latency by 40% through implementation of event-driven architecture\\n- Mentored 15+ senior developers in cloud-native development practices'},\n",
       "  {'title': 'Lead Software Engineer',\n",
       "   'company': 'DataFlow Systems',\n",
       "   'end_date': '2020',\n",
       "   'start_date': '2016',\n",
       "   'description': '- Architected and led development of distributed data processing platform handling 5TB daily\\n- Designed microservices architecture reducing deployment time by 65%\\n- Led migration of legacy monolith to cloud-native architecture\\n- Managed team of 8 engineers across 3 international locations'},\n",
       "  {'title': 'Senior Software Engineer',\n",
       "   'company': 'InnovateTech',\n",
       "   'end_date': '2016',\n",
       "   'start_date': '2013',\n",
       "   'description': '- Developed high-performance trading platform processing 100K transactions per second\\n- Implemented real-time analytics engine reducing processing latency by 75%\\n- Led adoption of container orchestration reducing deployment costs by 35%'}],\n",
       " 'technical_skills': {'skills': ['Architecture & Design',\n",
       "   'Microservices',\n",
       "   'Event-Driven Architecture',\n",
       "   'Domain-Driven Design',\n",
       "   'REST APIs',\n",
       "   'Cloud Platforms'],\n",
       "  'frameworks': ['AWS (Advanced)', 'Azure', 'Google Cloud Platform'],\n",
       "  'programming_languages': ['Java', 'Python', 'Go', 'JavaScript/TypeScript']},\n",
       " 'key_accomplishments': '- Co-inventor on three patents for distributed systems architecture\\n- Published paper on \"Scalable Microservices Architecture\" at IEEE Cloud Computing Conference 2022\\n- Keynote Speaker, CloudCon 2023: \"Future of Cloud-Native Architecture\"\\n- Regular presenter at local tech meetups and conferences'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have an agent that can extract structured data from resumes. \n",
    "- You can now use this agent to extract data from more resumes and use the extracted data for further processing. \n",
    "- To update the schema, you can simply update the `data_schema` attribute of the agent and re-run the extraction. \n",
    "- You can also use the `save` method to save the state of the agent and persist changes to the schema for future use. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
